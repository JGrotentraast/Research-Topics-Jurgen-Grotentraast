% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[review]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{tabularx}
\usepackage{float}
\usepackage{listings}
\usepackage{pgfgantt}
\usepackage{amsmath}
\usepackage{longtable}
\usepackage[
backend=biber,
style=numeric-comp,
]{biblatex}
\addbibresource{bibliography.bib}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Systematic approach to finding the most suitable Data warehouse approach and tooling for a situation}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Jurgen Grotentraast \\
  Student Data Science \& technology\\
  University of Twente \\
  j.grotentraast@student.utwente.nl\\}

\begin{document}
{\makeatletter\acl@finalcopytrue
  \maketitle
}
% \begin{abstract}
% Abstract
% \end{abstract}

\section{Introduction}
With the still-growing value of data in today's world, many organizations have invested in the development of a data warehouse. A data warehouse is used to store data differently to efficiently analyze business data \cite{gupta1997selection}. Data warehouses can be used for analyzing and improving business processes \cite{shahzad2009goal}, but also to get a better understanding of for example the financial situation of an organization \cite{lapura2018development}. A data warehouse utilizes historical data to show trends, averages, and bottlenecks in a process or production chain to show what areas can be improved. \\

A data warehouse captures data from one or multiple sources, transforms the data in such a way that aggregations on this data are easy and fast to execute, and finally loads this data into the data warehouse database. This process is called extract-transform-load (ETL). Over the years many tools and software solutions have been developed to aid people in this process. Some tools are purely programming libraries or extensions that help the user to achieve what they want \cite{Thomsen201821, Jensen202145, Biswas_programming2019267}, and others are more full-fledged software that can be used to build ETL pipelines with minimal coding. While major companies like Amazon, Microsoft, and Google have developed data warehouse solutions, these are often very expensive and require a subscription to their entire cloud platform to use them. However, over the last couple of years, open-source data warehouse tools have developed further and further \cite{nejres2015analysis}. This means that these tools now have the same functionality and even allow the user to build upon the tool themselves if something is missing. For example, the source of the data is not yet part of the tool, the user can build a custom connector through for example an API and still extract all the data they want. \\

Designing a data warehouse can be done in various ways. Each approach has its advantages and disadvantages. The approach that works best for a company or person depends on various factors including the use-case of the data warehouse and the background of the designer. Over the years these approaches have been developed further and further and new trends have emerged. As discussed in more detail further on, literature of the past years has extensively researched the trends of the approach of the design, development, implementation, and improvement of a data warehouse \cite{costa2018evaluating, kozmina2018information, golfarelli2017star, chandra2018comprehensive}. \\

\section{Problem statement}
With this rise in open-source tools and the continuously changing trends in the approach or methodology of the design, development, implementation, or improvement of data warehouses, a clear overview of currently available tools and current trends is missing. Literature up until now has shown trends that were emerging up until 2018/2019, but whether those trends are still relevant and what other trends have emerged since then is unknown. 

\subsection{Research questions}
Therefore, for this review, two main research questions were created. Answering these questions will give insight into current developments in open-source ETL tools as well as DW trends that have emerged over the past five years.

\begin{itemize}
    \item \textbf{RQ1}: What open-source tools are available?
    \item \textbf{RQ2}: What are current trends and approaches in the research, design, development, implementation, or improvement of a data warehouse?
\end{itemize}

\section{Related work}
\label{related}
The following paragraphs will focus on similar research that was found. The chapter will highlight their findings and how this study complements or differs from those findings. \\

For the tooling, one similar study was found. This study by Biswas et al.\cite{Biswas_programming2019267} compares different Python libraries that offer ETL capabilities. When the tools mentioned in this paper by Biswal et al. met the inclusion and exclusion criteria for the tooling as described in \ref{searchstrat:tools} these tools were also considered for this study, however, this study includes a more complete overview of the open-source tools available.\\

In terms of trends, more studies were found. First, there were studies published in 2018 reviewing trends up until then \cite{costa2018evaluating, kozmina2018information, golfarelli2017star, chandra2018comprehensive}. While each study had its take and focus, all these studies recognized the increase in data volume which resulted in a shift from traditional data warehousing to big data warehousing. Furthermore, these studies showed that the architecture of a data warehouse has also shifted over the years. Where the standard used to be a relational database with a clear structure, these studies show that the architectures up until 2018 also started to shift to incorporate more NoSQL capabilities as the data that these systems had to handle became more and more unstructured. Moreover, the designing of a data warehouse also shows several clear approaches that have emerged over the past years. The approaches were classified into five categories: data-driven, which starts the design face by analyzing the source data; requirement-driven, which starts at the other end, looking at the requirements from the end user; mixed approaches, which combine a data-driven and requirement-driven approach; query-based approaches, which start by defining the workload the DW should take care of; and finally, a pattern-based approach, which also start at the source data but look for multidimensional patterns. Lastly, these studies show that a data warehouse has to be able to handle more and more types of data from more and more different sources and should also be interoperable with more and more systems. These studies show trends and approaches up until 2018, therefore, this study will look at the trends from 2018 up until now. This study will show whether trends that started six or seven years from now are still relevant, or maybe completely new trends have emerged.\\

Second, S. Eom published a study on the current state and emerging trends regarding decision support systems, business intelligence, and data analysis \cite{eom2020dss}. These kinds of systems are often based on a data warehouse and therefore, trends in these systems might affect trends in the data warehousing. While this study slightly overlaps with the population chosen for this study, the study by Eom focuses more on the fields research in this direction is focused on as well as use cases of these kinds of systems. Whereas this study will focus more on trends in the design and implementation of a data warehouse. \\

Third and last, Dhaouadi et al. published a work on the classical approach and new trends in the design of the ETL process \cite{dhaouadi2022data}. The study identified six different classes of ETL modeling. The different classes were ETL process modeling based on UML, ontologies, MDA, graphical flow formalism (BPMN, CPN, YAWL, data visualization flow), ad hoc formalisms (conceptual constructs, CommonCube, EMD), and approaches for big data. The conclusion of this paper shows that ETL process modeling based on standard modeling languages like UML or BPMN were confirmed to be powerful methods as they standardize the ETL workflow design. ETL process modeling based on ontologies showed an easy identification of the schema of the data sources and DW. Furthermore, ontologies are most suitable for capturing the semantics of the domain model. However, mapping between different sources was considered an extremely complex task. Next, one advantage of MDA-based process modeling was the separation of business logic and technology by providing different layers that lead to interoperable, reusable, and portable software components and data models. The biggest advantage of these MDA-based methods was the automated transformations of models to implementations, which are done through automatic code generation from these models. One drawback of these automated transformations is the reliance on patterns and references to constantly updated libraries. The use of patterns also showed interesting results, as patterns allow for reusability of parts of the ETL process which reduces potential design errors in future parts. The work by Dhaouadi et al. is a perfect addition to the results found in this study. The focus of Dhaouadi et al. highlights the different approaches of a sub-area of data warehouse research. \\


\section{Methodology}
\label{methodology}
For both research questions a literature review was conducted, where the result for the first question was extended with a Google search. Both literature studies had a population of published studies from 2018 up to and including February 2024. All papers are written in English and are published in the field of computer science. For both literature studies, Scopus was used as the library of choice as it was indicated as the most comprehensive and user-friendly literature database \cite{harzing2016google, mongeon2016journal}.\\

\subsection{Search strategy}
\label{searchstrat}
For both literature studies, a search query was created through a process of trial and error to see which combination of keywords and query composition returned the best results. Next, inclusion and exclusion criteria were created, for the first RQ, this was done for both the research papers as well as for the tools that were found, for the second RQ this was only done for the papers. \\

\subsubsection{Tools}
\label{searchstrat:tools}
To answer the first research question, a combination of results from literature and Google were used. Since the goal of this research question is to find all available tools that are currently popular, using only literature was not an option as this would yield a very limited result and would not be a representative overview of the software that is available. The following search query was used: \\

\textbf{(open-source OR "open source") AND ("data warehouse" OR etl) AND (solution OR tool)}\\

The search was done in the title, abstract, and keywords of the study. The result was a set of 65 papers. These papers, however, were not all relevant. Therefore several inclusion criteria for the papers were set up. As mentioned before, the goal is to find tooling which should be considered in the proposed research. Therefore, the quality of the paper or the results that were obtained were not of importance for this study. Therefore, there are only two inclusion criteria.

\begin{itemize}
    \item \textbf{IC1}: The subject of the paper should be data warehousing or ETL.
    \item \textbf{IC2}: The paper should mention the tools that were used for the research.
\end{itemize}

These criteria were applied while reading the title and abstract of each paper, this resulted in 18 papers being left. These papers were carefully read to find any tool that was mentioned. This initial result was extended with tools found through a Google search. For this search the following queries were used:

\begin{itemize}
    \item open source data warehouse tools
    \item open source etl tools
\end{itemize}

The resulting pages included rankings of the so-called "best" ETL or data warehouse tool to forums discussing different possibilities for tools that can be used to develop and run ETL pipelines. The complete list of tools that were found can be seen in table \ref{table:tools}. However, not all of these tools should be taken into consideration for the proposed research. Therefore, new inclusion and exclusion criteria for the tools were formed.

\begin{itemize}
    \item \textbf{IC1}: The tool has to be open-source
    \item \textbf{IC2}: The source code should be accessible
    \item \textbf{IC3}: The latest release should be in or after 2020
    \item \textbf{EC1}: The tool is operating system specific
\end{itemize}

\subsubsection{Trends \& approaches}
\label{searchstrat:trends}
To answer the second research question, a second literature study was performed. The population was the same as for the first, except that this study was also limited to conference papers, articles, and book chapters. For this second part, the following search query was used:\\

\textbf{"data warehouse" AND  (design OR concept OR methodology)}\\

The search was again done in the title, abstract, and keywords of the study. The result was a set of 743 papers. These papers, however, were not all relevant. Therefore several inclusion criteria for the papers were set up. For this study, the quality of the papers did matter, as the goal of this part is to find current trends and problems found in research in the domain of, the design of, and the development of data warehouses. Therefore, the following inclusion criteria were formed:

\begin{itemize}
    \item \textbf{IC1}: The paper directly addresses a trend or problem with a solution or approach for the design, development, implementation, or improvement of a data warehouse
    \item \textbf{IC2}: The paper is peer-reviewed
    \item \textbf{IC3}: The paper is written in clear English
    \item \textbf{IC3}: The paper is available for download
\end{itemize}

An exclusion criterion was also formed to ensure the inclusion of only relevant information further.
\begin{itemize}
    \item \textbf{EC1}: The paper discusses the implementation of a data warehouse in a specific field without explicitly addressing and explaining a trend, problem, or approach in the design, development, implementation, or improvement of a data warehouse
\end{itemize}

After applying the above criteria while reading the title and abstract there were 231 papers left initially. Some papers were initially included but were rather ambiguous. These ambiguous cases were analyzed in further detail to ensure that all non-relevant papers were excluded, this further analysis resulted in 178 papers. These ambiguous cases included terms like "detailed description of design method" in the abstract, however, in the paper itself this only included what the star schema and ETL process looked like. Since this was not relevant to this literature study, these papers were still excluded.\\

\section{Results \& Discussion}
\label{results}
After gathering the papers for both studies, the inclusion and exclusion criteria could be applied to gather the relevant results. The following sections show the tools that were identified in the literature and that were found on Google as well as the trends and approaches that are currently of interest in research, design, and development regarding data warehouses. \\

\subsection{Tools}
\label{results:tools}
After applying the inclusion and exclusion criteria, several tools were dropped. The final list consists of 25 tools. The first IC resulted in Talend, StreamSets, and Keboola being dropped. While Talend has been one of the biggest names in the open-source data warehouse industry, its software is no longer open-source as of 31st of January 2024. Keboola and StreamSets were also dropped because only a part of their tool was open-source but required the non-open-source part to work. \\

The second IC resulted in the exclusion of Hevo, as the source code for their platform was not accessible at all nor could the open-source license the software falls under be found. The third criterion was created to ensure that the software is still being kept up to date in terms of security and modern technologies. Therefore, Scriptella was excluded for example. Scriptella’s latest release was in October 2019 and no new release has been announced or planned since then. The software also uses rather outdated technology and is therefore not seen as a future-proof solution. \\

\begin{table}[H]
\centering
    \begin{tabular}{|c|}
        \hline
        \textbf{Name of tool} \\ \hline
        Airbyte \\ \hline 
        Apache Airflow \\ \hline  
        Apache Beam \\ \hline  
        Apache Camel \\ \hline
        Apache Druid \cite{Dymora202363} \\ \hline  
        Apache Hadoop \cite{Song2018233, Yoo2019476, Dymora202363} \\ \hline  
        Apache Hive \cite{Yoo2019476, HouSu20221581, Dymora202363, Camacho-Rodríguez20191773} \\ \hline 
        Apache Hop \\ \hline  
        Apache Kafka \cite{Dymora202363} \\ \hline  
        Apache NiFi \\ \hline  
        Apache SeaTunnel \\ \hline  
        Apache Spark \cite{Song2018233} \\ \hline  
        CloudQuery \\ \hline  
        Dagster \\ \hline  
        DBT \\ \hline  
        {\color[HTML]{FE0000} Hevo Data \cite{Sreemathy20211650}} \\ \hline
        {\color[HTML]{FE0000} Keboola} \\ \hline
        Kestra \\ \hline
        Knime Analytics Platform \\ \hline
        Mage \\ \hline
        Meltano \\ \hline
        {\color[HTML]{FE0000} OpenXDMoD \cite{Dean2022}} \\ \hline
        Pentaho Community edition \cite{Sreemathy20211650, Fang2022, Zheng2023} \\ \hline
        Perfect \\ \hline
        PipelineWise \\ \hline
        Python libraries\text{*} \cite{Fissore2018267} \\ \hline
        R\_etl \cite{Biswas_programming2019267, Biswas_realtime202053} \\ \hline
        {\color[HTML]{FE0000} Scriptella \cite{Biswas_programming2019267, Biswas_realtime202053}} \\ \hline
        Singer\\ \hline
        {\color[HTML]{FE0000} StreamSets \cite{Sreemathy20211650}} \\ \hline
        {\color[HTML]{FE0000} Talend \cite{Espinoza2023, Sreemathy20211650}} \\ \hline
    \end{tabular}
    \caption{The complete list of tools that were found before applying the criteria. The tools that were excluded after applying the criteria are marked in red. \\
    \text{*}The Python libraries include: Ethereum-etl \cite{Camozzi2022}, Pygrametl\cite{Thomsen201821, Jensen202145, Biswas_programming2019267, Biswas_realtime202053}, Petl \cite{Biswas_programming2019267, Biswas_realtime202053}, and Luigi}
\label{table:tools}
\end{table}

The exclusion criterion resulted in Open XDMoD being excluded. Open XDMoD is an ETL tool that can only run on Linux-based systems. This is not necessarily a bad thing, however, in this review, the tools should depend on areas like use cases and employee knowledge not on the operating system someone is running. \\

The tools that were no longer included after applying the above criteria are marked in red in table \ref{table:tools}. If the tool was found in a paper, this paper is mentioned in table \ref{table:tools} as well. The papers of Yu et al. and Spengler et al. \cite{Yu2022, Spengler2020415} are not mentioned in the table as these papers described the process of creating their own ETL tool from scratch. While the paper of Fissore et al. is mentioned in the table, it should be noted that this paper did not use any specific tool or library. Instead, they used basic Python functionalities to take care of their ETL process. However, since these have very specific purposes these are not taken into consideration. A more comprehensive overview of the included tools can be found in appendix \ref{appendix:tools}. This appendix shows some basic information about each tool including if the tool uses a specific programming language or is more low/no code; the size of the GitHub contributors and the number of stars the repository has; if the tool has a non-open-source paid option with more functionalities; and an optional small note for interesting capabilities, weaknesses or other noteworthy findings of the tool. \\ 


\subsection{Trends \& approaches}
\label{results:trends}
The following paragraphs will discuss the resulting papers for the trends and approaches in more detail. This section is divided into several subsections based on the category the paper covers.

\subsubsection{Data warehouse architecture \& design approaches}
\label{results:trends:design}
One of the biggest trends was the rise of the Data Lake and the Data Lakehouse. 

Several papers \cite{Ramadhani202188, Himami2021146, Wahyudi2019, Rahutomo2018128} mention and describe the use of the Kimball methodology. Even though this method of design was already introduced in 2002 \cite{kimball2011data}, it is still a popular methodology for designing a data warehouse. The methodology consists of the following nine steps: define process; determine grain; identify and confirm dimensions; determine facts; store calculations in a fact table; complete dimension table; determine the duration of the database to be created; seek a slowly changing dimension; and lastly determine the physical design. Some studies have used the methodology of Kimball as a basis to improve the design cycle of a data warehouse by introducing several hybrid approaches \cite{Takács20201}. \\

Another methodology that was mentioned in the papers was the HEFESTO 2.0 method \cite{uvidia2017moving}. Like the Kimball methodology, the business process is the leading factor that indicates how a data warehouse should be designed. Uvidia et al. combined the HEFESTO 2.0 methodology with the nine-step knowledge discovery in database (KDD) process in their study. Where HEFESTO focuses on the data warehouse design, KDD focuses on data mining. The HEFESTO 2.0 methodology consists of four stages. First, the requirements need to be analyzed by identifying the questions that need to be answered, identifying the indicators and perspectives, and creating a conceptual model. Next, an OLTP analysis has to be performed. Which includes forming and creating the indicators, determining the granularity, and expanding the conceptual model. The third stage tackles the DW logical model. This stage includes the creation of the data schema with the dimension and fact tables and the different views. Finally, the last stage is data integration, where data is loaded into the data warehouse. This process is then extended by Uvidia et al. with five more steps for data mining to create knowledge discovery in a data warehouse. \\

Raman et al. mentioned Brewer's rule in their paper, implications of Brewer's rule in data warehouse design \cite{raman2023implications}. Brewer's rule, also known as the CAP theorem, states that a distributed system cannot simultaneously provide all three guarantees. The three guarantees are consistency, which means that a write operation on one node should be replicated on all other nodes to keep data consistent across all nodes; availability, which states that even if nodes in the system are down a request should always return the correct data; and finally partition tolerance, which states that the system should always work even if network partitions prevent nodes from communicating. This rule helps explain the trade-offs required in the design of a distributed system. It helps designers pick the attributes that are important to their use case.\\

Finally on the topic of overall data warehouse design approaches, Letrache et al. propose an approach to design and exploit green data warehouses as well as several guidelines and best practices to achieve this green data warehouse \cite{letrache2018green}. A green data warehouse is a sustainable and efficient approach to data warehousing that reduces IT waste where the resulting data warehouse uses as little storage as possible and is as future-proof as possible such that no new system has to be created in the foreseeable future. \\

\subsubsection{Schema design}

\subsubsection{ETL}

\subsubsection{Types of data}
In terms of data, there have been several data sources that have been gaining popularity which alter the design of the data warehouse or the ETL process. The first type of data that is gaining popularity is Linked Open Data (LOD). Data warehouses that incorporate LOD are typically called semantic data warehouses (SDW). These SDWs have been studied in detail over the last few years \cite{Berkani2020397, Berkani20181ETL, Khouri2018, Khouri20181, khouri2019data, Soussi2023762}. All these papers identified that traditional data warehouses are not suitable to handle LOD. Both the data model and ETL process should be adapted to handle this data. However, as most prominently described in the article on the "contribution of linked open data to augment a traditional data warehouse" \cite{Berkani2020397}, LOD adds a lot of value to a data warehouse. In this same article, a value-driven approach for designing a data warehouse and ETL process is proposed. N. Soussi proposed a new method to optimize the columnar design of a data warehouse to better incorporate LOD \cite{Soussi2023762}. In "LOD for data warehouses: managing the ecosystem co-evolution" \cite{Khouri2018}, Khouri et al. highlight the importance of keeping track of co-evolutions among LOD. Since the data is linked, changes in one source affect other sources which adds a new level of difficulty keeping data consistent. Berkani et al. propose a new method to select a materialized view in their paper "ETL-aware materialized view selection in semantic data stream warehouses" \cite{Berkani20181ETL}. In "Consolidation of BI efforts in the LOD era for African context" \cite{Khouri20181}, Khouri et al. propose a requirement-driven approach for designing an SDW. Khouri et al. explicitly showed the need for these new approaches in their study "Data cube is dead, long life to data cube in the age of web data" \cite{khouri2019data}. In which they highlight that a traditional design no longer suffices if LOD is to be incorporated. \\

The second type of data that is rising in popularity is IoT data. IoT devices often generate huge amounts of data and, depending on the application, might be connected to many other IoT devices. For example, in the paper "Data Warehouse design for soil nutrients with IoT based data sources" where the source data consists of several sensors each measuring certain qualities of the soil \cite{Rahman2019181}. This study showed how to deal with IoT devices as sources by designing the ETL process in a way that can handle the incoming message from all the different sensors. Plazas et al. show different methodologies to design a data warehouse that incorporates IoT data \cite{Plazas202084}. Lastly, on IoT data, Liu et al. looked into a new data management paradigm for Artificial Intelligence of Things (AIoT) \cite{Liu202334}. They present a survey on IoT lakehouse. Like papers mentioned in \ref{results:trends:design} they discuss characteristics, benefits, and challenges of a Lakehouse, however this time with regards to IoT data. They conclude that an IoT lakehouse can enable scalable, reliable, and efficient data analysis for IoT and AIoT. \\

The last type of data that was found in the literature is data on the movement of objects, also referred to as trajectory data. Zekri et al. in their paper "Integrating trajectory data in the warehousing chain: a new way to handle the trajectory ELT process" \cite{Zekri2018380} show that this kind of data requires a new way of modeling the ETL process. Azaiez et al. in their paper "Trajectory ETL modeling" \cite{Azaiez2018353} on the other hand first presented the drawback of using an ETL process and instead proposed an ELT process to handle trajectory data. Furthermore, Garani et al. developed a data warehouse capable of utilizing this trajectory data \cite{Garani202388}. This paper described the decisions they made in their design based on the usage of trajectory data. Lastly, Brisaboa et al. analyzed the problem of representing and managing semantic trajectories compactly and efficiently \cite{RodriguezBrisaboa2020113}. They present a data structure called the Semantrix, a semantic matrix supporting different types of queries.\\

Closely related to trajectory data, one paper also mentioned the challenges faced in creating a data warehouse using Geospatial data \cite{Ferro2019221}. In this study, Ferro et al. deal with these challenges by normalizing parts of the geospatial fields to increase query performance and reduce storage costs.\\

With all these different types of data that pose challenges to the design of a data warehouse, one trend is the shift to NoSQL databases. N. Soussi has shown this with her proposal to optimize the columnar design of a data warehouse model which was implemented using a NoSQL database \cite{Soussi2023762}. Bouaziz et al. have shown in both their articles "Towards data warehouse from open data: Case of COVID-19" and "Design a data warehouse schema from document-oriented database" that such a NoSQL database is very useful in dealing with various types of data \cite{Bouaziz2019221, Bouaziz2021129}. N. Artamonov has shown that a NoSQL approach for building a data store with a dynamic structure is just as feasible as building one with a relational or object-oriented approach \cite{Artamonov2019794}. Banerjee et al. propose an ontology-driven conceptual model for a NoSQL-based DW solution \cite{Banerjee2021162}. The proposed model defines the formal semantics of the related DW concepts in NoSQL-based solutions. Oukhouya et al. conducted a synthetic review of literature on the design of hybrid storage architectures that merge traditional systems with NoSQL systems \cite{Oukhouya2023332}. They analyzed the implementations of these hybrid architectures and found three types of conceptual models, relational, UML, and multidimensional. They also found that UML models are the most descriptive. The multidimensional approach is most suited to automate the transition from conceptual to physical NoSQL model. Khalil et al. took a deeper look into a specific type of NoSQL database in their article "A graph-oriented framework for online analytical processing" \cite{Khalil2022547}. The propose a modeling approach for implementing a graph-based data warehouse using labeled nodes and edges. The ability of graph technology to handle highly interconnected data made it a very suitable candidate for interactive analysis. \\

\subsubsection{Performance improvement}


\section{Future research \& approach}
\label{approach}
The information that is gathered with this review is a perfect stepping stone for future research. With the tooling, trends, and approaches that were found in this review, a framework can be created that can be used to determine which tools are most suitable for the data warehousing needs of a business or person. Before this framework can begin taking shape, the gathered information needs to be structured. The different tools should be analyzed in more depth to determine their technological and business strengths such that this can be matched with the needs of a user and the approach they want to take. For this, the tools first need to be categorized and matched with different trends and approaches that this tool would be a good fit for. For creating the framework it must also be determined how to shape the framework. For example, the framework can be a series of questions that narrow down the tools, but can also be a clear overview of the categorization from which users can see which tool would fit their needs. After this is determined the actual framework can be created. \\

The framework should then also be tested. This will be done in the form of a case study for Topicus .Finance. The focus within Topicus .Finance lies on three main sectors, namely pension and wealth \cite{pension},  mortgages \cite{mortgages}, and lending \cite{businesslending}. Within each of these sectors, Topicus .Finance has multiple software applications for individuals or companies. In this study, one of these software applications for the lending side will be used as the basis for a case study. The application is called Fyndoo \cite{fyndoo}, and is used by almost all major banks and many smaller banks and other financial institutions that offer lending services in the Netherlands. Fyndoo is a software application that streamlines the lending application process for both the financial institution and the applicant. Within Fyndoo Topicus .Finance wants to give their clients more insight into their processes as well as help them with the reporting they need to do to "De Nederlandsche Bank" (DNB), which in turn reports to the European Central Bank (ECB). These problems can both be solved with the use of a data warehouse which means this case is very suitable as a test for the framework. \\

After the framework is applied on Topicus .Finance's situation, a data warehouse will be designed and built with the tools that were suggested by the framework. This implementation will also require insight into the client of Topicus .Finance. Therefore, interviews will be conducted with these clients to gather information on the processes these clients wish to monitor and which reporting needs they still have. From these interviews, the Key Performance Indicators (KPIs) can be found which will help in the design of the data warehouse. Lastly, the implemented solution has to be evaluated. First with the clients to see if the new features of Fyndoo are helpful. Second the framework has to be evaluated with Topicus .Finance to see if the implemented solution is also a good fit for them. With both of these evaluations, it should be possible to see if the framework is useful or not.

\section{Planning}
\label{planning}
Figure \ref{gantt_chart} shows the preliminary planning for this future research. The first eleven weeks will be used for completing all analysis components of this research. In these eleven weeks, the interviews for the case study will already be completed and analyzed. Since the interviews will be dependent on the availability of the clients of Topicus .Finance it is preferable to have these planned early to stay on schedule. Parallel to these interviews, the tooling and approaches must be analyzed in more depth as discussed in the approach in section \ref{approach}. Furthermore, decisions have to be made regarding the tooling to be used for the implementation of the data warehouse. Moreover, the different nature of each of these tasks gives a balance in the type of work that is required to complete them. This can help with motivation and focus as performing the same task for a long time can be very exhausting and can lead to a lack of focus. The next six weeks will be used to design and implement the data warehouse. The final four weeks are used to receive feedback on the developed solution and the framework, finish writing the report, and prepare for the presentation. These weeks also allow for a little extension if parts carry on for longer than expected.

\begin{figure*}[t]

    \begin{center}
    \begin{ganttchart}[y unit title=0.4cm,
    y unit chart=0.5cm,
    vgrid,hgrid, 
    vrule/.style={very thick, green},
    vrule label font=\bfseries,
    title label anchor/.style={below=-1.6ex},
    title left shift=.05,
    title right shift=-.05,
    title height=1,
    progress label text={},
    bar/.append style={rounded corners=3pt},
    bar height=0.7,
    group right shift=0,
    group top shift=.6,
    group height=.3]{1}{21}
    %labels
    \gantttitle{Week}{21}\\
    \gantttitlelist{1,...,21}{1} \\\\
    %tasks
    \ganttbar[bar/.append style={fill=purple}]{Prepare and plan interviews}{1}{2} \\
    \ganttbar[bar/.append style={fill=violet}]{Conduct and analyse interviews}{3}{10} \\
    \ganttbar[bar/.append style={fill=orange}]{Match tools with practice}{2}{7}\\
    \ganttbar[bar/.append style={fill=magenta}]{Categorize tools}{2}{7}\\
    \ganttbar[bar/.append style={fill=cyan}]{Create framework}{6}{9}\\
    \ganttbar[bar/.append style={fill=yellow}]{Apply framework to Topicus}{10}{11} \\ 
    \ganttbar[bar/.append style={fill=blue}]{Design and implement data warehouse}{12}{17}\\ 
    \ganttbar[bar/.append style={fill=red}]{Evaluate solution \& framework}{18}{18} \\
    \ganttbar[bar/.append style={fill=lime}]{Extension}{19}{20} \\
    \ganttbar[bar/.append style={fill=pink}]{Presentation}{20}{21} \\
    \ganttbar[bar/.append style={fill=teal}]{Writing}{2}{21} \\
    \ganttvrule{Green light}{17}
    \end{ganttchart}
    \caption{Gantt chart of preliminary planning final project}
    \label{gantt_chart}
    \end{center}
\end{figure*}

% Entries for the entire Anthology, followed by custom entries
\printbibliography

\onecolumn
\appendix

\section{Overview of tools}
\label{appendix:tools}
\begin{table}[H]
\centering
    \begin{tabular}[c]{|p{2cm}|p{4cm}|p{4cm}|p{1.5cm}|p{4cm}|}
    \hline
    \textbf{Name} & \textbf{UI/Code} & \textbf{Git rating/community size} & \textbf{Paid option} & \textbf{Features} \\ \hline
    Airbyte & UI, Terraform,and API & 13.2k stars, 863 contributors & Yes & DBT for transformations \\ \hline
    Apache Airflow & UI and Python & 33.5k stars, 2804 contributors & No &  \\ \hline
    Apache Beam & Python, Java, GO, typescript, Scala, SQL, YAML & 7.4k stars, 1170 contributors & No & Multi-language pipelines \\ \hline
    Apache Camel & Java & 5.2k stars, 1029 contributors & No & Integration tool with ETL \\ \hline
    Apache Druid & Web UI with SQL queries & 13.1k stars, 591 contributors & No &  \\ \hline
    Apache Hadoop & MapReduce & 14.2k stars, 573 contributors & No & Cluster computation \\ \hline
    Apache Hive & CLI & 5.3k stars, 372 contributors & No &  \\ \hline
    Apache Hop & GUI + web, CLI tools & 813 stars, 70 contributors & No & Based on Pentaho \\ \hline
    Apache Kafka & Java and Scala & 26.9k stars, 1105 contributors & No & Real-time streaming \\ \hline
    Apache NiFi & Web UI, & 4.2k stars, 459 contributors & No &  \\ \hline
    Apache SeaTunnel & CLI & 7k stars, 252 contributors & No &  \\ \hline
    Apache Spark & Python, Java, R, Scala, SQL & 37.9k stars, 2042 contributors & No & Big data analysis \\ \hline
    CloudQuery & CLI & 5.4k stars, 142 contributors & Yes & Transformation with DBT \\ \hline
    Dagster & Python and Web UI & 9.7k stars, 380 contributors & Only paid & Integrates with DBT and Airbyte \\ \hline
    DBT & CLI, SQL & 8.5k stars, 290 contributors & Yes & Integrates well with other tools \\ \hline
    Kestra & Localhost GUI with IDE & 5.4k stars, 194 contributors & Yes & Integrates with Airbyte and DBT \\ \hline
    Knime & GUI & 144 stars, 17 contributors on GitHub & Yes & Has own community platform \\ \hline
    Mage & GUI or own IDE, Python, R and SQL & 6.6k stars, 92 contributors & No &  \\ \hline
    Meltano & CLI & 1.5k stars, 120 contributors & No & Transformation with DBT \\ \hline
    Pentaho Community edition & Low-code GUI & 7.2k stars, 221 contributors & No &  \\ \hline
    Perfect & Python + monitoring UI & 14.1k stars, 227 contributors & Yes &  \\ \hline
    PipelineWise & CLI & 597 stars, 45 contributors & No & Requires Singer \\ \hline
    Python libraries\text{*} & Code & NA & No & Scheduling with cron \\ \hline
    R\_etl & R & NA & No & Dedicated R package \\ \hline
    Singer & CLI & $\sim$1.5k stars, $\sim$30 contributors & No & PipelineWise scheduling and monitoring \\ \hline
    \end{tabular}
    \caption{\text{*}The Python libraries include: Ethereum-etl, Pygrametl, Petl, Luigi}
\end{table}

If the tool uses code as its main way of building ETL pipelines, the programming language is mentioned. If the application is more low/no code, this is indicated in terms of the UI that is used. Sometimes both options are possible while other times a UI is only for monitoring the pipelines, not for building.

\end{document}
