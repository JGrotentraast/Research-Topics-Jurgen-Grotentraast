% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[review]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{tabularx}
\usepackage{float}
\usepackage{listings}
\usepackage{pgfgantt}
\usepackage{amsmath}
\usepackage{longtable}
\usepackage[
backend=biber,
style=numeric-comp,
]{biblatex}
\addbibresource{bibliography.bib}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Systematic approach to finding the most suitable Data warehouse approach and tooling for a situation}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Jurgen Grotentraast \\
  Student Data Science \& technology\\
  University of Twente \\
  j.grotentraast@student.utwente.nl\\}

\begin{document}
{\makeatletter\acl@finalcopytrue
  \maketitle
}
\begin{abstract}
Abstract
\end{abstract}

\section{Introduction}
With the still-growing value of data in today's world, many organizations have invested in the development of a data warehouse (DW). A data warehouse is used to store data differently to efficiently analyze business data \cite{gupta1997selection}. Data warehouses can be used for analyzing and improving business processes \cite{shahzad2009goal}, but also to get a better understanding of for example the financial situation of an organization \cite{lapura2018development}. A data warehouse utilizes historical data to show trends, averages, and bottlenecks in a process or production chain to show what areas can be improved. \\

A data warehouse captures data from one or multiple sources, transforms the data in such a way that aggregations on this data are easy and fast to execute, and finally loads this data into the data warehouse database. This process is called extract-transform-load (ETL). Over the years many tools and software solutions have been developed to aid people in this process. Some tools are purely programming libraries or extensions that help the user to achieve what they want \cite{Thomsen201821, Jensen202145, Biswas_programming2019267}, and others are more full-fledged software that can be used to build ETL pipelines with minimal coding. While major companies like Amazon, Microsoft, and Google have developed data warehouse solutions, these are often very expensive and require a subscription to their entire cloud platform to use them. However, over the last couple of years, open-source data warehouse tools have developed further and further \cite{nejres2015analysis}. This means that these tools now have the same functionality and even allow the user to build upon the tool themselves if something is missing. For example, the source of the data is not yet part of the tool, the user can build a custom connector through for example an API and still extract all the data they want. \\

Designing a data warehouse can be done in various ways. Each approach has its advantages and disadvantages. The approach that works best for a company or person depends on various factors including the use-case of the data warehouse and the background of the designer. Over the years these approaches have been developed further and further and new trends have emerged. As discussed in more detail further on, literature of the past years has extensively researched the trends of the approach of the design, development, implementation, and improvement of a data warehouse \cite{costa2018evaluating, kozmina2018information, golfarelli2017star, chandra2018comprehensive}. \\

\section{Problem statement}
With this rise in open-source tools and the continuously changing trends in the approach or methodology of the design, development, implementation, or improvement of data warehouses, a clear overview of currently available tools and current trends is missing. Literature up until now has shown trends that were emerging up until 2018/2019, but whether those trends are still relevant and what other trends have emerged since then is unknown. 

\subsection{Research questions}
Therefore, for this review, two main research questions were created. Answering these questions will give insight into current developments in open-source ETL tools as well as DW trends that have emerged over the past five years.

\begin{itemize}
    \item \textbf{RQ1}: What open-source tools are available?
    \item \textbf{RQ2}: What are current trends and approaches in the research, design, development, implementation, or improvement of a data warehouse?
\end{itemize}

\section{Related work}
\label{related}
The following paragraphs will focus on similar research that was found. The chapter will highlight their findings and how this study complements or differs from those findings. \\

For the tooling, one similar study was found. This study by Biswas et al.\cite{Biswas_programming2019267} compares different Python libraries that offer ETL capabilities. When the tools mentioned in this paper by Biswal et al. met the inclusion and exclusion criteria for the tooling as described in \ref{searchstrat:tools} these tools were also considered for this study, however, this study includes a more complete overview of the open-source tools available.\\

In terms of trends, more studies were found. First, there were studies published in 2018 reviewing trends up until then \cite{costa2018evaluating, kozmina2018information, golfarelli2017star, chandra2018comprehensive}. While each study had its take and focus, all these studies recognized the increase in data volume which resulted in a shift from traditional data warehousing to big data warehousing. Furthermore, these studies showed that the architecture of a data warehouse has also shifted over the years. Where the standard used to be a relational database with a clear structure, these studies show that the architectures up until 2018 also started to shift to incorporate more NoSQL capabilities as the data that these systems had to handle became more and more unstructured. Moreover, the designing of a data warehouse also shows several clear approaches that have emerged over the past years. The approaches were classified into five categories: data-driven, which starts the design face by analyzing the source data; requirement-driven, which starts at the other end, looking at the requirements from the end user; mixed approaches, which combine a data-driven and requirement-driven approach; query-based approaches, which start by defining the workload the DW should take care of; and finally, a pattern-based approach, which also start at the source data but look for multidimensional patterns. Lastly, these studies show that a data warehouse has to be able to handle more and more types of data from more and more different sources and should also be interoperable with more and more systems. These studies show trends and approaches up until 2018, therefore, this study will look at the trends from 2018 up until now. This study will show whether trends that started six or seven years ago are still relevant, or maybe completely new trends have emerged.\\

Second, S. Eom published a study on the current state and emerging trends regarding decision support systems, business intelligence, and data analysis \cite{eom2020dss}. These kinds of systems are often based on a data warehouse and therefore, trends in these systems might affect trends in the data warehousing. While this study slightly overlaps with the population chosen for this study, the study by Eom focuses more on the fields research in this direction is focused on as well as use cases of these kinds of systems. Whereas this study will focus more on trends in the design and implementation of a data warehouse. \\

Third and last, Dhaouadi et al. published a work on the classical approach and new trends in the design of the ETL process \cite{dhaouadi2022data}. The study identified six different classes of ETL modeling. The different classes were ETL process modeling based on UML, ontologies, MDA, graphical flow formalism (BPMN, CPN, YAWL, data visualization flow), ad hoc formalisms (conceptual constructs, CommonCube, EMD), and approaches for big data. The conclusion of this paper shows that ETL process modeling based on standard modeling languages like UML or BPMN were confirmed to be powerful methods as they standardize the ETL workflow design. ETL process modeling based on ontologies showed an easy identification of the schema of the data sources and DW. Furthermore, ontologies are most suitable for capturing the semantics of the domain model. However, mapping between different sources was considered an extremely complex task. Next, one advantage of MDA-based process modeling was the separation of business logic and technology by providing different layers that lead to interoperable, reusable, and portable software components and data models. The biggest advantage of these MDA-based methods was the automated transformations of models to implementations, which are done through automatic code generation from these models. One drawback of these automated transformations is the reliance on patterns and references to constantly updated libraries. The use of patterns also showed interesting results, as patterns allow for reusability of parts of the ETL process which reduces potential design errors in future parts. The work by Dhaouadi et al. is a perfect addition to the results found in this study. The focus of Dhaouadi et al. highlights the different approaches of a sub-area of data warehouse research. \\


\section{Methodology}
\label{methodology}
For both research questions a literature review was conducted, where the result for the first question was extended with a Google search. Both literature studies had a population of published studies from 2018 up to and including February 2024. All papers are written in English and are published in the field of computer science. For both literature studies, Scopus was used as the library of choice as it was indicated as the most comprehensive and user-friendly literature database \cite{harzing2016google, mongeon2016journal}.\\

\subsection{Search strategy}
\label{searchstrat}
For both literature studies, a search query was created through a process of trial and error to see which combination of keywords and query composition returned the best results. Next, inclusion and exclusion criteria were created, for the first RQ, this was done for both the research papers as well as for the tools that were found, for the second RQ this was only done for the papers. \\

\subsubsection{Tools}
\label{searchstrat:tools}
To answer the first research question, a combination of results from literature and Google were used. Since the goal of this research question is to find all available tools that are currently popular, using only literature was not an option as this would yield a very limited result and would not be a representative overview of the software that is available. The following search query was used: \\

\textbf{(open-source OR "open source") AND ("data warehouse" OR etl) AND (solution OR tool)}\\

The search was done in the title, abstract, and keywords of the study. The result was a set of 65 papers. These papers, however, were not all relevant. Therefore several inclusion criteria for the papers were set up. As mentioned before, the goal is to find tooling which should be considered in the proposed research. Therefore, the quality of the paper or the results that were obtained were not of importance for this study. Therefore, there are only two inclusion criteria.

\begin{itemize}
    \item \textbf{IC1}: The subject of the paper should be data warehousing or ETL.
    \item \textbf{IC2}: The paper should mention the tools that were used for the research.
\end{itemize}

These criteria were applied while reading the title and abstract of each paper, this resulted in 18 papers being left. These papers were carefully read to find any tool that was mentioned. This initial result was extended with tools found through a Google search. For this search the following queries were used:

\begin{itemize}
    \item open source data warehouse tools
    \item open source etl tools
\end{itemize}

The resulting pages included rankings of the so-called "best" ETL or data warehouse tool to forums discussing different possibilities for tools that can be used to develop and run ETL pipelines. The complete list of tools that were found can be seen in table \ref{table:tools}. However, not all of these tools should be taken into consideration for the proposed research. Therefore, new inclusion and exclusion criteria for the tools were formed.

\begin{itemize}
    \item \textbf{IC1}: The tool has to be open-source
    \item \textbf{IC2}: The source code should be accessible
    \item \textbf{IC3}: The latest release should be in or after 2020
    \item \textbf{EC1}: The tool is operating system specific
\end{itemize}

\subsubsection{Trends \& approaches}
\label{searchstrat:trends}
To answer the second research question, a second literature study was performed. The population was the same as for the first, except that this study was also limited to conference papers, articles, and book chapters. For this second part, the following search query was used:\\

\textbf{"data warehouse" AND  (design OR concept OR methodology)}\\

The search was again done in the title, abstract, and keywords of the study. The result was a set of 743 papers. These papers, however, were not all relevant. Therefore several inclusion criteria for the papers were set up. For this study, the quality of the papers did matter, as the goal of this part is to find current trends and problems found in research in the domain of, the design of, and the development of data warehouses. Therefore, the following inclusion criteria were formed:

\begin{itemize}
    \item \textbf{IC1}: The paper directly addresses a trend or problem with a solution or approach for the design, development, implementation, or improvement of a data warehouse
    \item \textbf{IC2}: The paper is peer-reviewed
    \item \textbf{IC3}: The paper is written in clear English
    \item \textbf{IC3}: The paper is available for download
\end{itemize}

An exclusion criterion was also formed to ensure the inclusion of only relevant information further.
\begin{itemize}
    \item \textbf{EC1}: The paper discusses the implementation of a data warehouse in a specific field without explicitly addressing and explaining a trend, problem, or approach in the design, development, implementation, or improvement of a data warehouse
\end{itemize}

After applying the above criteria while reading the title and abstract there were 231 papers left initially. Some papers were initially included but were rather ambiguous. These ambiguous cases were analyzed in further detail to ensure that all non-relevant papers were excluded, this further analysis resulted in 171 papers. These ambiguous cases included terms like "detailed description of design method" in the abstract, however, in the paper itself this only included what the star schema and ETL process looked like. Since this was not relevant to this literature study, these papers were still excluded.\\

\section{Results \& Discussion}
\label{results}
After gathering the papers for both studies, the inclusion and exclusion criteria could be applied to gather the relevant results. The following sections show the tools that were identified in the literature and that were found on Google as well as the trends and approaches that are currently of interest in research, design, and development regarding data warehouses. \\

\subsection{Tools}
\label{results:tools}
After applying the inclusion and exclusion criteria, several tools were dropped. The final list consists of 25 tools. The first IC resulted in Talend, StreamSets, and Keboola being dropped. While Talend has been one of the biggest names in the open-source data warehouse industry, its software is no longer open-source as of 31st of January 2024. Keboola and StreamSets were also dropped because only a part of their tool was open-source but required the non-open-source part to work. \\

The second IC resulted in the exclusion of Hevo, as the source code for their platform was not accessible at all nor could the open-source license the software falls under be found. The third criterion was created to ensure that the software is still being kept up to date in terms of security and modern technologies. Therefore, Scriptella was excluded for example. Scriptella’s latest release was in October 2019 and no new release has been announced or planned since then. The software also uses rather outdated technology and is therefore not seen as a future-proof solution. \\

\begin{table}[H]
\centering
    \begin{tabular}{|c|}
        \hline
        \textbf{Name of tool} \\ \hline
        Airbyte \\ \hline 
        Apache Airflow \\ \hline  
        Apache Beam \\ \hline  
        Apache Camel \\ \hline
        Apache Druid \cite{Dymora202363} \\ \hline  
        Apache Hadoop \cite{Song2018233, Yoo2019476, Dymora202363} \\ \hline  
        Apache Hive \cite{Yoo2019476, HouSu20221581, Dymora202363, Camacho-Rodríguez20191773} \\ \hline 
        Apache Hop \\ \hline  
        Apache Kafka \cite{Dymora202363} \\ \hline  
        Apache NiFi \\ \hline  
        Apache SeaTunnel \\ \hline  
        Apache Spark \cite{Song2018233} \\ \hline  
        CloudQuery \\ \hline  
        Dagster \\ \hline  
        DBT \\ \hline  
        {\color[HTML]{FE0000} Hevo Data \cite{Sreemathy20211650}} \\ \hline
        {\color[HTML]{FE0000} Keboola} \\ \hline
        Kestra \\ \hline
        Knime Analytics Platform \\ \hline
        Mage \\ \hline
        Meltano \\ \hline
        {\color[HTML]{FE0000} OpenXDMoD \cite{Dean2022}} \\ \hline
        Pentaho Community edition \cite{Sreemathy20211650, Fang2022, Zheng2023} \\ \hline
        Perfect \\ \hline
        PipelineWise \\ \hline
        Python libraries\text{*} \cite{Fissore2018267} \\ \hline
        R\_etl \cite{Biswas_programming2019267, Biswas_realtime202053} \\ \hline
        {\color[HTML]{FE0000} Scriptella \cite{Biswas_programming2019267, Biswas_realtime202053}} \\ \hline
        Singer\\ \hline
        {\color[HTML]{FE0000} StreamSets \cite{Sreemathy20211650}} \\ \hline
        {\color[HTML]{FE0000} Talend \cite{Espinoza2023, Sreemathy20211650}} \\ \hline
    \end{tabular}
    \caption{The complete list of tools that were found before applying the criteria. The tools that were excluded after applying the criteria are marked in red. \\
    \text{*}The Python libraries include: Ethereum-etl \cite{Camozzi2022}, Pygrametl\cite{Thomsen201821, Jensen202145, Biswas_programming2019267, Biswas_realtime202053}, Petl \cite{Biswas_programming2019267, Biswas_realtime202053}, and Luigi}
\label{table:tools}
\end{table}

The exclusion criterion resulted in Open XDMoD being excluded. Open XDMoD is an ETL tool that can only run on Linux-based systems. This is not necessarily a bad thing, however, in this review, the tools should depend on areas like use cases and employee knowledge not on the operating system someone is running. \\

The tools that were no longer included after applying the above criteria are marked in red in table \ref{table:tools}. If the tool was found in a paper, this paper is mentioned in table \ref{table:tools} as well. The papers of Yu et al. and Spengler et al. \cite{Yu2022, Spengler2020415} are not mentioned in the table as these papers described the process of creating their own ETL tool from scratch. While the paper of Fissore et al. is mentioned in the table, it should be noted that this paper did not use any specific tool or library. Instead, they used basic Python functionalities to take care of their ETL process. However, since these have very specific purposes these are not taken into consideration. A more comprehensive overview of the included tools can be found in appendix \ref{appendix:tools}. This appendix shows some basic information about each tool including if the tool uses a specific programming language or is more low/no code; the size of the GitHub contributors and the number of stars the repository has; if the tool has a non-open-source paid option with more functionalities; and an optional small note for interesting capabilities, weaknesses or other noteworthy findings of the tool. \\ 


\subsection{Trends \& approaches}
\label{results:trends}
The following paragraphs will discuss the resulting papers for the trends and approaches in more detail. This section is divided into several subsections based on the category the paper covers.

\subsubsection{Data Lake \& Data Warehouse architecture}
\label{results:trends:design}
One of the biggest trends was the rise of the Data Lake (DL) and the Data Lakehouse (DLH), both evolutions of a DW. Many of the papers found on data lakes cover the comparison with a traditional DW and discuss the benefits and challenges that DLs and DLHs bring to the table \cite{Li2023, Chen2022405, Singh2022530, Orescanin20211242, Liu2020, Kachaoui2019, Giebler2019179, Ravat2019304}. The results of these papers show that a DL can save structured, semi-structured, and unstructured raw data, whereas a DW can only store structured processed data. A DW processes a schema on write whereas a DL does this on read. DW storage is expensive for a large volume whereas a DL is low-cost. A DW has a fixed configuration and is therefore not agile, whereas a DL is highly agile with an adjustable configuration. The security of a DW is mature whereas that of a DL is maturing. Furthermore, a DW is mostly used by business professionals whereas a DL is used by data scientists and other experts in the field. Moreover, a DW is therefore also more used for business intelligence and reporting whereas a DL is often also used for data science and machine learning. Finally, as a way of summarizing the differences, a DW is used to analyze historical data which can therefore be saved in a fixed format whereas a DL is used for advanced data analytics and therefore needs more freedom to achieve more insights. A DLH is a combination of the two, it can store both processed and raw structured, semi-structured and unstructured data, can process the schema on write and on read, is low cost for storage, highly agile, both mature and maturing security, and used by the whole business environment. \\

Data Lakes also make use of different storages \cite{Singh2022530}. Most commonly a DL uses a file storage like the Hadoop Distributed File System (HDFS) which is very good at handling large quantities of data that often reside in a DW or DL. There are also single data store DLs, that use a graph-based information model for example. Next, there are cloud-based DLs, which come with all the benefits that cloud storage has to offer. Lastly, there are polystore systems, these systems use multiple data stores each with its purpose. For example, a relational model in a MySQL database, a document-based store in MongoDB, and a graph database in Neo4j. With this combination of stores, every type of data can be stored easily. \\

For the sake of usability of the data in a DL, the data does need to be processed somewhere. Ravat et al. propose a four-zone architecture for this purpose \cite{Ravat2019304}. The four zones include the raw data zone, in which all types of data are ingested without processing, which is exactly the strong suit of a DL; the process zone, where the data can be transformed according to the requirements of the user; the access zone, which stores all processed data; and finally the governance zone, which is an overlapping zone that applies data governance on all the other zones to ensure data security, quality, life-cycle, access, and metadata management. \\

Giebler et al. also explain this zone architecture but also dive into an alternative, the pond architecture \cite{Giebler2019179}. This architecture consists of a raw data pond, which stores all raw data. The data is then transformed and divided into three ponds depending on their characteristics. There is the analog data pond, the application data pond, and the textual data pond. Finally, there is the archival data pond for data that is no longer used. \\

The biggest challenge identified in these papers is ensuring a DL does not turn into a so-called data swamp \cite{Giebler2019179}. This problem is highly related to metadata management as it is a result of the absence of descriptive metadata. Without proper metadata management, erroneous data, duplicate data, and incorrect data can easily be ingested into the DL. The veracity of the data ingested into the DL cannot be ensured and therefore the data quality can rapidly degrade turning the entire DL untrustworthy. \\

Several studies have tackled these issues. Two studies by Sawadogo et al. looked into metadata management for DLs. In their study "Metadata systems for data lakes: models and features" they propose a generic graph-based model for metadata management \cite{Sawadogo2019440}. They identified six key features that a metadata system must provide. They propose a new graph-based metadata model called MEDAL based on the notion of object and typology of metadata in three categories. These categories are intra-object, inter-object, and global metadata. An object is represented by a hypernode containing nodes that correspond to the versions and representations of an object. Oriented edges linking the nodes represent transformation and update operations. Hypernodes can be linked in several ways: edges represent similarity links and hyperarcs are used to model parenthood relationships and object groupings. Finally, global resources are also present, in the form of knowledge bases, indexes, or event logs. The second study is the predecessor of the first. In this study, the theoretical concept of the metadata model is laid out as a start to analyzing unstructured data like textual data \cite{Sawadogo2019558}. \\

In the paper "Life and Death of Data Lakes: Preserving Data Usability and Responsible Governance" the problem of data governance in a DL is discussed \cite{Derakhshannia2019302}. This paper tackles the issue of a DL turning into a data swamp by looking into data usability. They propose preliminary directions for removing useless data based on comparisons with supply chain management and natural lakes on one hand and on forgetting functions and decision support systems on the other hand. The last study tackling the issues of a DL is a paper called "Combining data lake and data wrangling for ensuring data quality in CRIS" \cite{Azeroual20223}. Current Research Information Systems (CRIS) are designed to store and manage data about conducted research. In their paper, Azeroual et al. propose a way of combining a DL with data wrangling. Data wrangling is a more complex way of cleaning your data. Instead of removing irrelevant data, data wrangling is a method where the goal is to make the data relevant again. The steps included in data wrangling are: discover, structure, clean, enrich, validate, and publish. This method is very suitable to be combined with a DL and will ensure that the DL does not turn into a data swamp full of useless data. \\

Next to papers tackling the challenges, Behm et al. already propose an improved query engine for lakehouse systems called Photon \cite{Behm20222326}. The new query engine's native design solved many issues of scalability and performance present in their previously developed JVM-based execution engine. The vectorized processing model of Photon enables rapid development, rich metrics reporting, and micro-adaptive execution for handling the unstructured raw data that is ubiquitous in Dls and DLHs. \\

Two papers were found that looked into the implementation of a DL. Sawadogo et al. developed a DL system that combines the analysis of textual documents with the analysis of tabular data \cite{Sawadogo202188}. The basis of their implementation is an approach they named AUDAL, which includes an extensive metadata system to allow querying and analyzing the data lake and supports more features than the other state-of-the-art DL implementations. The second study looked into a DL as an alternative to a traditional DW to support the decision-making process in cycling promotion \cite{Schering2022783}. They investigated the difference between a traditional DW and a DL to see if a DL would be a suitable choice for managing the bicycle infrastructure. The heterogeneous nature of the data they have makes a DL a very suitable alternative. \\

Finally, two papers already suggest a new alternative to a DL architecture, a data mesh architecture \cite{Vlasiuk2023183, Machado2021263}. A data mesh architecture will make data the true center of an organization. It shifts away from a monolithic architecture into a distributed platform. The data mesh architecture consists of mesh nodes. Each mesh node holds a data product. Data as a product implies that there is a set of characteristics that are held by that data. This data product consists of three main components, code, (meta)data, and infrastructure. The code component in turn encompasses three distinct segments: data pipeline, applications that allow access to the data and metadata, and the code used for access policies. The data component encapsulates the actual data. This data can be of different types but for it to be used, the respective metadata part has to be associated with the data. The infrastructure component allows access to the data and metadata, as well as running the code related to the data product. To avoid inefficient duplication of efforts, the data mesh architecture should be designed without considering the business domain concept. This will allow the use of the same infrastructural capabilities across different domains. The governance model applied in a data mesh consists of two relevant dimensions. First, the achievement of the measures imposed at a global level, and second, the respect for the autonomy of the various data domains that compose the mesh. \\

\subsubsection{Data warehouse design approach}
Several papers \cite{Ramadhani202188, Himami2021146, Wahyudi2019, Rahutomo2018128} mention and describe the use of the Kimball methodology. Even though this design method was already introduced in 2002 \cite{kimball2011data}, it is still a popular methodology for designing a data warehouse. The methodology consists of the following nine steps: define process; determine grain; identify and confirm dimensions; determine facts; store calculations in a fact table; complete dimension table; determine the duration of the database to be created; seek a slowly changing dimension; and lastly determine the physical design. Some studies have used the methodology of Kimball as a basis to improve the design cycle of a data warehouse by introducing several hybrid approaches \cite{Takács20201}. \\

Another methodology that was mentioned in the papers was the HEFESTO 2.0 method \cite{uvidia2017moving}. Like the Kimball methodology, the business process is the leading factor that indicates how a data warehouse should be designed. Uvidia et al. combined the HEFESTO 2.0 methodology with the nine-step knowledge discovery in database (KDD) process in their study. Where HEFESTO focuses on the data warehouse design, KDD focuses on data mining. The HEFESTO 2.0 methodology consists of four stages. First, the requirements need to be analyzed by identifying the questions that need to be answered, identifying the indicators and perspectives, and creating a conceptual model. Next, an OLTP analysis has to be performed. Which includes forming and creating the indicators, determining the granularity, and expanding the conceptual model. The third stage tackles the DW logical model. This stage includes the creation of the data schema with the dimension and fact tables and the different views. Finally, the last stage is data integration, where data is loaded into the data warehouse. This process is then extended by Uvidia et al. with five more steps for data mining to create knowledge discovery in a data warehouse. \\

Raman et al. mentioned Brewer's rule in their paper, implications of Brewer's rule in data warehouse design \cite{raman2023implications}. Brewer's rule, also known as the CAP theorem, states that a distributed system cannot simultaneously provide all three guarantees. The three guarantees are consistency, which means that a write operation on one node should be replicated on all other nodes to keep data consistent across all nodes; availability, which states that even if nodes in the system are down a request should always return the correct data; and finally partition tolerance, which states that the system should always work even if network partitions prevent nodes from communicating. This rule helps explain the trade-offs required in the design of a distributed system. It helps designers pick the attributes that are important to their use case.\\

Finally on the topic of overall data warehouse design approaches, Letrache et al. propose an approach to design and exploit green data warehouses as well as several guidelines and best practices to achieve this green data warehouse \cite{letrache2018green}. A green data warehouse is a sustainable and efficient approach to data warehousing that reduces IT waste where the resulting data warehouse uses as little storage as possible and is as future-proof as possible such that no new system has to be created in the foreseeable future. \\

\subsubsection{Schema design}
The first modeling concept regarding database schema design is the Data Vault method. Four papers were found that discussed the Data Vault methodology or used it in the design of a DW. Two papers discuss what the Data Vault methodology entails \cite{Gluchowski2021277, Jaksic2020813}. The Data Vault concept ensures scalability and auditing capability through consistent model separation of keys (hub tables), descriptive attributes (satellite tables), and relationship information (link tables). Furthermore, conceptual clarity, flexibility, and adaptability are also ensured with this method. The three different types of tables, hub, satellite, and link, each have their role. The hub tables represent the core objects of the respective business logic. Next to a surrogate key, there is always a business key that is unique throughout the company. This can be something like a customer number or product number. A hub does not contain relationship information or descriptive attributes. They are created once and not changed after that, which allows ETL processes to be limited to insert statements. Satellite tables store all descriptive attributes of hubs and links. A hub or link can also have multiple satellites. If a hub or link has multiple satellite tables then these are usually divided according to the type of data, the frequency of changes, or the source. Satellite tables also only need insertions as changes are managed by adding a new row and keeping the old record for historical data analysis. Link tables map all relationships between hubs. The data vault method does produce more artifacts than a dimensional or normalized data model, since the representatives of the individual object types hub, like and satellite have great structural similarities, the ETL process shows great automation potential. \\

This Data Vault methodology has also been compared to the more traditional star/snowflake model in terms of storage size and query performance with Apache Hadoop and Hive \cite{Grigoriev2021147}. For this comparison, the TPC-H benchmark test was used, which includes a data model and several data sources that are loaded periodically. The results show that a traditional snowflake model is much better for query execution when Apache Hive is used, as the MapReduce approach of Hive has trouble with multiple table joins, which is a necessity for the Data Vault model. This result shows that the Data Vault method might not work well with every use case. However, Kuznetcov et al. showcase that the Data Vault model is a useful design method as it allows you to design a data warehouse of an information system in such a way that its metamodel is semantically related to the domain of the system \cite{Kuznetcov2019}. \\

Next, several papers looked into ontology-based schema design. By first creating an ontology of the domain, the facts and dimensions and their attributes become clear. Zekri et al. used this methodology to extract decision-making dimensions of the Bid Process Information System \cite{Zekri2019285, Zekri20191190}. The current work is focused on the design phase in which they have shown how the use of a multidimensional ontology can be beneficial in the design phase. In the paper "A software prototype for multidimensional design of data warehouses using ontologies" Zekri et al. extended their knowledge of ontology-based design by combining the multidimensional ontology with a domain ontology and requirements ontology to show that his combination is more significant to the design process than these sources alone \cite{Zekri2019273}. \\

The research on ontology-based design has also been used to automatically construct a star schema for a DW. Hajji et al. have developed a plug-in for Eclipse modeling project which automatically generates a DW model from an ontology or the model of a relational database \cite{Hajji2020613}. This last part has been done before, however, their solution has the advantage of offering the end-user the possibility to customize the generated models to their needs. Yang et al. also focused on automating this design \cite{Yang2022173}. They propose a machine learning-based method for detecting OLAP measures from tabular data. Their current results are promising but only focus on numeric values, they want to continue their study to also include textual data. Pizarro et al. have gone a step further and proposed a model that can generate DW design based on textual requirements \cite{Pizarro202213}. They achieve this by selecting nouns, verbs, and words that express measurement units which the designer will try to match with source data and create a star schema. Sanprasit et al. have combined all these techniques to automatically model a star schema for a DW with the assistance of subject domain ontologies \cite{Sanprasit2021, Sanprasit2021518}. They first identify KPIs using NLP. Next, they developed a probability density machine learning model to create all entities and attributes for the star schema. They compared their approach with existing approaches and saw that their approach was much faster and more accurate. Finally, they applied their new model to a real-world case. The current version only supports star schema construction, and can not work with unstructured data. In future work, they will add support for snowflake and galaxy schemas and unstructured data. Lastly, Huo et al. developed an automated method for designing a multidimensional database schema for a DW from relational aggregation queries \cite{Huo2020337}. They use a series of given queries over a relational database schema to automatically generate a multidimensional database schema. They continue their research by also addressing the schema evolution problem and to see if they can automatically update the schema if the current one can not answer a new query. This evolution problem is also tackled by Taktak et al., who chose a model-driven approach to automate the propagation of the evolution of the source data schema and the evolution of decision-makers' requirements \cite{Taktak2018401}. To do so, they defined three evolution models, one for the source data, one for the requirements, and one for the DW. They also defined rules that implemented the transformation process for the passage between these models. To validate their approach, a software prototype was developed which offers model-to-model transformations and model-to-text transformations. \\

Sautot et al. and Nogueira et al both propose a method for semi-automatic design. These studies were published before the rise of fully automated design. Sautot et al. extend a previous study that created dimensions hierarchies originating from factual data \cite{Sautot202128}. This previous study created so many hierarchies that the resulting multidimensional model became too complex. Therefore, they propose to reduce these hierarchies from a set of algorithms which will still help in the design of the DW and enrich the dimensions as was done in the previous study. Nogueira et al. propose seven steps to design a DW model \cite{Nogueira202085}. Each step is accompanied by a flowchart that outlines step-by-step how to model this rule. The result is a model that is a general model that works best as a first effort that should be validated and optimized by a use case expert or data engineer. \\

While automated design has shown a rise in popularity, there are also still manual approaches developed that improve the final resulting DW design. Takács et al. propose a method and concrete designing tool that can make the designing phase of a DW much more efficient by letting the user and the designer work together more closely and draw up the conceptual plan together \cite{Takács20201}. This way the requirements of the end user can be ensured much better. Elamin et al. propose a similar approach, where the requirements of the end-user play a vital role \cite{Elamin2019}. They propose the elicitation of the business requirements from users at different levels which should be represented in a unified format. These requirements are then reconciled with the star schema that is generated from the data source. If there are inconsistencies between the unified requirements and the star schema, the star schema can be updated to ensure a good design of the DW. \\

Both Ain El Hayat et al. and Gosain et al. looked into the bi-temporal versioning of a DW schema. In bitemporal versioning of schema, a new version of the schema is created when the desired revisions are applied to the latest version of the schema, provided it satisfies the validity condition. This form of versioning uses bitemporal time (VT and TT) to timestamp every version of the schema. The proposed schema revision can affect only the present version or the versions that have some overlap with the time interval specified for the revisions. It allows retro- and pro-active schema modifications and helps in keeping a record of such changes \cite{Gosain2018357}. Gosain et al. took the first step in creating this bi-temporal versioning of the schema, which Ain El Hayat developed further to create a complete temporal DW that uses bi-temporal versioning for both the schema and the data \cite{AinElHayat2019314}. The bi-temporal versioning of the data is done by using their proposed metadata model. Phugtua-Eng et al. propose the use of a temporal DW to solve the slowly changing dimension (SCD) problem \cite{Phungtua-Eng2022214}. The SCD problem is a problem that arises with the infrequent change of a dimension. Since a DW is used to analyze historical data, this change in a dimension should be tracked such that results before and after this change are correctly reflected. For this purpose, Phungtua-Eng proposes the use of a temporal DW. Their results show that several ways of implementing a temporal DW can deal with this SCD problem. \\

Another design concept that was mentioned in the literature was a data cube. A data cube is nothing more than a set of dimensions that allows for answering complex questions that relational databases can not \cite{Bantug2018}. Bantug et al. reviewed their lessons learned after using data cubes for a while \cite{Bantug2018}. Next to answering questions relational databases can not, using a data cube also allows businesses to see relationships between dimensions by moving up and down the hierarchy of the data. Lastly, they found that data cubes support faster analysis which is also less prone to errors. Phogat et al. focused their research on the computation of a data cube \cite{Phogat2023697}. The complete set of cubes in a DW is called a lattice. According to Phogat et al., a traditional lattice has some drawbacks, such as adding more dimensions will almost double the size of the structure since all new dimensions must connect the structure's bottom and upper bound dimensions. Therefore they propose to use the hyperlattice concept for computing the data cubes. Their results show that using this hyperlattice concept reduces the size of the structure and improves the computation time. Djiroun et al. focused their research on constructing cubes from existing cubes based on the needs of the decision-maker \cite{Djiroun2019783}. Their approach constructs new data cubes consisting of data from several existing cubes. With their developed tool a user can complete or enrich an existing cube based on their needs or the user can select measures and dimensions for which the tool will provide similar measures to enhance the cube. Lastly, Brahmi et al. proposed a method to mine the data cubes through Generalized Association Rule (GAR) mining \cite{Brahmi2019153}. They introduce a new method for mining GARs that takes advantage of the attributes of a data cube. Their algorithm uses hierarchies in a post-mining step to generalize association rules which greatly reduces execution time, memory usage, and the overwhelming number of generated rules that other approaches yield which hampers efficient perception. \\

Rocha et al. tackled the problem of designing a schema for a combination of geographic, socioeconomic, and image data in a healthcare setting \cite{Rocha202085}. They designed three star schemas to see if these data types should be stored together or separately. They introduce an extension for processing analytical queries with similarity search predicates to evaluate the query performance of these three different schemas. The results showed that storing the similarity search factors separately and linking one table to another respecting the granularity of the attributes provided better performance in the majority of cases. Their extension of analytical queries also proved the importance of these types of data to the healthcare decision-making process. \\

Then there were several novel design studies. First, Bouaziz et al. developed an approach to design a DW schema from a NoSQL database \cite{Bouaziz2019221}. Their approach includes identifying the type of NoSQL database, for example, document-oriented or graph-based; extracting the schema from each record; defining a structure graph for each extracted schema; identifying multidimensional concepts in the structure graphs; and designing the DW schema, this last step can be automated. Second, Karkouda et al. present a new schema for securely hosting and querying a DW in the cloud \cite{Karkouda20186}. Their schema consists of three large parts that each secure a level of security (confidentiality, integrity, and availability). The schema is based on a homomorphic encryption algorithm. The weakness in this schema is overcome by using homomorphic privacy based on multi-cloud providers and perturbation values. Thirdly, Artamonov et al. propose a design method for a dynamically structured data store \cite{Artamonov2019794}. This new model tackles the limitations of the rational approach to data mining. Lastly, Sakka et al. proposed a new methodology for DW design where volunteers are involved in the definition of the analysis needs \cite{Sakka2018286}. The idea is similar to crowdsourcing systems like Wikipedia, where anybody can add, delete, or modify the content until an agreement is reached. The approach uses a confidence level with which volunteers can indicate their level of confidence in the ideas they proposed. The approach was tested on a case study and showed promise, but requires further testing on cases with more conflicting ideas to validate the true effectiveness of the approach. \\

\subsubsection{ETL}
One major field of research is creating a near real-time DW. The papers that were found on this subject all found the bottleneck to be the ETL process. Muddasir et al. broadly discuss three approaches to achieving near real-time ETL \cite{MohammedMuddasir2018436}. The three discussed approaches are a metadata management-based approach, a change data capture-based approach, and a parallel ETL partition-based approach. For each approach, several methods were tested. Metadata management reduces development time and helps in answering queries relating to relations. Change data capture reduces the amount of data that needs to be moved. Processing data in parallel also reduces the ETL time, but it requires that the data is properly partitioned. Panfilov et al. also looked into achieving resilient near real-time ETL by using metadata \cite{Panfilov2021139}. They propose a metadata framework creation and management of different types of ETL processes. The metadata framework automates typical ETL tasks and ensures an efficient implementation of ETL pipelines such that near real-time ETL can be achieved. \\

Like Muddasir et al., Chandra et al. also looked into change data capture (CDC) to achieve near real-time ETL \cite{Chandra2018}. They tested different CDC methods to see which one performs best. Their results show that it depends on the type of data. The best CDC for flat files, hierarchical data structures, and network data structures is a method based on the timestamp on the file, while for relational data structures and binary relational data structures a CDC method based on database triggers. \\

Another approach to near real-time ETL is proposed by Ghosh et al. They propose the use of virtual data warehouses to reduce query fetching time \cite{Ghosh2021585}. Their approach consists of two phases, in the first phase the goal is to identify common business cases and trends to divide the main DW into virtual sub-DWs each only containing relevant information on a specific case. If this case is then queried, the result can be fetched much faster. The second phase is used for cases where no match is found in the virtual DWs. If an abnormal case occurs, the closest virtual DW is chosen based on the z-score to handle the request by enriching data in that virtual DW. In the meantime, this abnormal situation is tracked. If it occurs frequently or is a permanent request, a new virtual DW is created for that situation. Similarly, Berkani et al. looked at combining materialized view selection with ETL processes \cite{Berkani20181ETL}. They tested the ideal phase in an ETL process to implement their newly proposed approach to dynamic materialized view selection. Their results show that combining these steps greatly improves the runtime of an ETL process. \\

For near real-time ETL one more approach was proposed in the literature. De Assis Vilela et al. proposed a novel solution to perform a soft real-time ETL process based on non-intrusive and reactive concepts \cite{DeAssisVilela2021556}. They propose Data Magnet, a new architecture to perform ETL processes in real-time DW environments. Data Magnet is non-intrusive, meaning it does not need to access the operational sources to get data of interest. Instead, it receives data from these sources using the tag concept. A tag is an indicator that an item of data will be used in the future in a DW for the decision-making process. The tag is placed on an item of data from operational sources and a data warehouse from a data warehousing environment. Now Data Magnet can receive the item of data from operational sources and automatically identify the target data warehouse that has an interest in storing data that is associated with a tag. Lastly, Data Magnet is reactive, meaning Data Magnet reacts to an insert event in a source by using a publish/subscribe system. \\

Muddasir et al. highlight an important problem with near real-time ETL, namely refreshment anomalies, and propose a solution for this problem. Their work focuses on three main causes for refreshment anomalies caused by real-time ETL \cite{MohammedMuddasir2020545}. The three main causes for anomalies that were identified are the interleaving of queries, delayed updates, and outer joins. They conclude that if an analysis query is put on hold while there is a concurrent update on the data warehouse, the waiting time of the query could be reduced if loads are performed on temporary replica tables. \\



data quality/validation \cite{Wrembel20223, Ali2020, Sreemathy20191183, Amuthabala2019233} \\
dynamic ETL \cite{BadiuzzamanBiplob2022243} \\
semantic ETL \cite{DebNath202185} \\
Topic dimensions \cite{Walha2019203, Walha2021374} \\
ETL tool development \cite{Wang2020} \\
quality of objectives \cite{Saebao2020539} \\
data discovery for ETL \cite{Madhikerrni2019174} \\
BPMN ETL \cite{Awiti2019299} \\
XML interchange format \cite{Awiti2019427} \\
SOA approach to ETL \cite{Oliveira2019204} \\
ECL-TL \cite{Pan2018275} \\
TOETL \cite{Teixeira2018225} \\
Era of Variety \cite{Berkani201898} \\
BigDimETL \cite{Mallek2018798}


\subsubsection{Types of data}
In terms of data, there have been several data sources that have been gaining popularity which alter the design of the data warehouse or the ETL process. The first type of data that is gaining popularity is Linked Open Data (LOD). Data warehouses that incorporate LOD are typically called semantic data warehouses (SDW). These SDWs have been studied in detail over the last few years \cite{Berkani2020397, Berkani20181ETL, Khouri2018, Khouri20181, khouri2019data, Soussi2023762}. All these papers identified that traditional data warehouses are not suitable to handle LOD. Both the data model and ETL process should be adapted to handle this data. However, as most prominently described in the article on the "contribution of linked open data to augment a traditional data warehouse" \cite{Berkani2020397}, LOD adds a lot of value to a data warehouse. This same article proposes a value-driven approach for designing a data warehouse and ETL process. N. Soussi proposed a new method to optimize the columnar design of a data warehouse to better incorporate LOD \cite{Soussi2023762}. In "LOD for data warehouses: managing the ecosystem co-evolution" \cite{Khouri2018}, Khouri et al. highlight the importance of keeping track of co-evolutions among LOD. Since the data is linked, changes in one source affect other sources which adds a new level of difficulty in keeping data consistent. Berkani et al. propose a new method to select a materialized view in their paper "ETL-aware materialized view selection in semantic data stream warehouses" \cite{Berkani20181ETL}. In "Consolidation of BI efforts in the LOD era for African context" \cite{Khouri20181}, Khouri et al. propose a requirement-driven approach for designing an SDW. Khouri et al. explicitly showed the need for these new approaches in their study "Data cube is dead, long life to data cube in the age of web data" \cite{khouri2019data}. In which they highlight that a traditional design no longer suffices if LOD is to be incorporated. \\

The second type of data that is rising in popularity is IoT data. IoT devices often generate huge amounts of data and, depending on the application, might be connected to many other IoT devices. For example, in the paper "Data Warehouse design for soil nutrients with IoT based data sources" where the source data consists of several sensors each measuring certain qualities of the soil \cite{Rahman2019181}. This study showed how to deal with IoT devices as sources by designing the ETL process in a way that can handle the incoming message from all the different sensors. Plazas et al. show different methodologies to design a data warehouse that incorporates IoT data \cite{Plazas202084}. Lastly, on IoT data, Liu et al. looked into a new data management paradigm for Artificial Intelligence of Things (AIoT) \cite{Liu202334}. They present a survey on IoT lakehouse. Like papers mentioned in \ref{results:trends:design} they discuss characteristics, benefits, and challenges of a Lakehouse, however this time with regards to IoT data. They conclude that an IoT lakehouse can enable scalable, reliable, and efficient data analysis for IoT and AIoT. \\

The last type of data that was found in the literature is data on the movement of objects, also referred to as trajectory data. Zekri et al. in their paper "Trajectory ETL modeling" \cite{Zekri2018380} show that this kind of data requires a new way of modeling the ETL process. Azaiez et al. in their paper "Integrating trajectory data in the warehousing chain: a new way to handle the trajectory ELT process" \cite{Azaiez2018353} on the other hand first presented the drawback of using an ETL process and instead proposed an ELT process to handle trajectory data. Furthermore, Garani et al. developed a data warehouse capable of utilizing this trajectory data \cite{Garani202388}. This paper described the decisions they made in their design based on the usage of trajectory data. Lastly, Brisaboa et al. analyzed the problem of representing and managing semantic trajectories compactly and efficiently \cite{RodriguezBrisaboa2020113}. They present a data structure called the Semantrix, a semantic matrix supporting different types of queries. They tested their data structure by generating huge amounts of synthetic realistic trajectories and evaluated Semantrix with those data to have a good idea about its space needs and its efficiency when answering different types of queries. \\

Closely related to trajectory data, one paper also mentioned the challenges faced in creating a data warehouse using Geospatial data \cite{Ferro2019221}. In this study, Ferro et al. deal with these challenges by normalizing parts of the geospatial fields to increase query performance and reduce storage costs.\\

With all these different types of data that pose challenges to the design of a data warehouse, one trend is the shift to NoSQL databases. N. Soussi has shown this with her proposal to optimize the columnar design of a data warehouse model implemented using a NoSQL database \cite{Soussi2023762}. Bouaziz et al. have shown in both their articles "Towards data warehouse from open data: Case of COVID-19" and "Design a data warehouse schema from document-oriented database" that such a NoSQL database is very useful in dealing with various types of data \cite{Bouaziz2019221, Bouaziz2021129}. N. Artamonov has shown that a NoSQL approach for building a data store with a dynamic structure is just as feasible as building one with a relational or object-oriented approach \cite{Artamonov2019794}. Banerjee et al. propose an ontology-driven conceptual model for a NoSQL-based DW solution \cite{Banerjee2021162}. The proposed model defines the formal semantics of the related DW concepts in NoSQL-based solutions. Oukhouya et al. conducted a synthetic review of literature on the design of hybrid storage architectures that merge traditional systems with NoSQL systems \cite{Oukhouya2023332}. They analyzed the implementations of these hybrid architectures and found three types of conceptual models, relational, UML, and multidimensional. They also found that UML models are the most descriptive. The multidimensional approach is most suited to automate the transition from conceptual to physical NoSQL model. Khalil et al. took a deeper look into a specific type of NoSQL database in their article "A graph-oriented framework for online analytical processing" \cite{Khalil2022547}. They propose a modeling approach for implementing a graph-based data warehouse using labeled nodes and edges. The ability of graph technology to handle highly interconnected data made it a very suitable candidate for interactive analysis. \\

\subsubsection{Performance improvement}
\label{performance}
web etl \cite{AbdAl-Rahman2023765}\\

\section{Future research \& approach}
\label{approach}
The information that is gathered with this review is a perfect stepping stone for future research. With the tooling, trends, and approaches that were found in this review, a framework can be created that can be used to determine which tools are most suitable for the data warehousing needs of a business or person. Before this framework can begin taking shape, the gathered information needs to be structured. The different tools should be analyzed in more depth to determine their technological and business strengths such that this can be matched with the needs of a user and the approach they want to take. Furthermore, the tools should be tested to see if they can handle certain trends that have emerged in the design and development of DWs to ensure the tools serve as a future-proof solution. For creating the framework it must also be determined how to shape the framework. For example, the framework can be a series of questions that narrow down the tools, but can also be a clear overview of the categorization from which users can see which tool would fit their needs. After this is determined the actual framework can be created. \\

The framework should then also be tested. This will be done in the form of a case study for Topicus .Finance. The focus within Topicus .Finance lies on three main sectors, namely pension and wealth \cite{pension},  mortgages \cite{mortgages}, and lending \cite{businesslending}. Within each of these sectors, Topicus .Finance has multiple software applications for individuals or companies. In this study, one of these software applications for the lending side will be used as the basis for a case study. The application is called Fyndoo \cite{fyndoo}, and is used by almost all major banks and many smaller banks and other financial institutions that offer lending services in the Netherlands. Fyndoo is a software application that streamlines the lending application process for both the financial institution and the applicant. Within Fyndoo Topicus .Finance wants to give their clients more insight into their processes as well as help them with the reporting they need to do to "De Nederlandsche Bank" (DNB), which in turn reports to the European Central Bank (ECB). These problems can both be solved with the use of a data warehouse which means this case is very suitable as a test for the framework. \\

After the framework is applied on Topicus .Finance's situation, a data warehouse will be designed and built with the tools that were suggested by the framework. This implementation will also require insight into the client of Topicus .Finance. Therefore, interviews will be conducted with these clients to gather information on the processes these clients wish to monitor and which reporting needs they still have. From these interviews, the Key Performance Indicators (KPIs) can be found which will help in the design of the data warehouse. Lastly, the implemented solution has to be evaluated. First with the clients to see if the new features of Fyndoo are helpful. Second the framework has to be evaluated with Topicus .Finance to see if the implemented solution is also a good fit for them. With both of these evaluations, it should be possible to see if the framework is useful or not.

\begin{figure*}[ht]
    \begin{center}
    \begin{ganttchart}[y unit title=0.4cm,
    y unit chart=0.5cm,
    vgrid,hgrid, 
    vrule/.style={very thick, green},
    vrule label font=\bfseries,
    title label anchor/.style={below=-1.6ex},
    title left shift=.05,
    title right shift=-.05,
    title height=1,
    progress label text={},
    bar/.append style={rounded corners=3pt},
    bar height=0.7,
    group right shift=0,
    group top shift=.6,
    group height=.3]{1}{21}
    %labels
    \gantttitle{Week}{21}\\
    \gantttitlelist{1,...,21}{1} \\\\
    %tasks
    \ganttbar[bar/.append style={fill=purple}]{Prepare and plan interviews}{1}{2} \\
    \ganttbar[bar/.append style={fill=violet}]{Conduct and analyse interviews}{3}{10} \\
    \ganttbar[bar/.append style={fill=orange}]{Match tools with practice}{2}{7}\\
    \ganttbar[bar/.append style={fill=magenta}]{Categorize tools}{2}{7}\\
    \ganttbar[bar/.append style={fill=cyan}]{Create framework}{6}{9}\\
    \ganttbar[bar/.append style={fill=yellow}]{Apply framework to Topicus}{10}{11} \\ 
    \ganttbar[bar/.append style={fill=blue}]{Design and implement data warehouse}{12}{17}\\ 
    \ganttbar[bar/.append style={fill=red}]{Evaluate solution \& framework}{18}{18} \\
    \ganttbar[bar/.append style={fill=lime}]{Extension}{19}{20} \\
    \ganttbar[bar/.append style={fill=pink}]{Presentation}{20}{21} \\
    \ganttbar[bar/.append style={fill=teal}]{Writing}{2}{21} \\
    \ganttvrule{Green light}{17}
    \end{ganttchart}
    \caption{Gantt chart of preliminary planning final project}
    \label{gantt_chart}
    \end{center}
\end{figure*}

\section{Planning}
\label{planning}
Figure \ref{gantt_chart} shows the preliminary planning for this future research. The first eleven weeks will be used for completing all analysis components of this research. In these eleven weeks, the interviews for the case study will already be completed and analyzed. Since the interviews will be dependent on the availability of the clients of Topicus .Finance it is preferable to have these planned early to stay on schedule. Parallel to these interviews, the tooling and approaches must be analyzed in more depth as discussed in the approach in section \ref{approach}. Furthermore, decisions have to be made regarding the tooling to be used for the implementation of the data warehouse. Moreover, the different nature of each of these tasks gives a balance in the type of work that is required to complete them. This can help with motivation and focus as performing the same task for a long time can be very exhausting and can lead to a lack of focus. The next six weeks will be used to design and implement the data warehouse. The final four weeks are used to receive feedback on the developed solution and the framework, finish writing the report, and prepare for the presentation. These weeks also allow for a little extension if parts carry on for longer than expected.

% Entries for the entire Anthology, followed by custom entries
\printbibliography
\onecolumn
\appendix

\section{Overview of tools}
\label{appendix:tools}
\begin{table}[H]
\centering
    \begin{tabular}[c]{|p{2cm}|p{4cm}|p{4cm}|p{1.5cm}|p{4cm}|}
    \hline
    \textbf{Name} & \textbf{UI/Code} & \textbf{Git rating/community size} & \textbf{Paid option} & \textbf{Features} \\ \hline
    Airbyte & UI, Terraform,and API & 13.2k stars, 863 contributors & Yes & DBT for transformations \\ \hline
    Apache Airflow & UI and Python & 33.5k stars, 2804 contributors & No &  \\ \hline
    Apache Beam & Python, Java, GO, typescript, Scala, SQL, YAML & 7.4k stars, 1170 contributors & No & Multi-language pipelines \\ \hline
    Apache Camel & Java & 5.2k stars, 1029 contributors & No & Integration tool with ETL \\ \hline
    Apache Druid & Web UI with SQL queries & 13.1k stars, 591 contributors & No &  \\ \hline
    Apache Hadoop & MapReduce & 14.2k stars, 573 contributors & No & Cluster computation \\ \hline
    Apache Hive & CLI & 5.3k stars, 372 contributors & No &  \\ \hline
    Apache Hop & GUI + web, CLI tools & 813 stars, 70 contributors & No & Based on Pentaho \\ \hline
    Apache Kafka & Java and Scala & 26.9k stars, 1105 contributors & No & Real-time streaming \\ \hline
    Apache NiFi & Web UI, & 4.2k stars, 459 contributors & No &  \\ \hline
    Apache SeaTunnel & CLI & 7k stars, 252 contributors & No &  \\ \hline
    Apache Spark & Python, Java, R, Scala, SQL & 37.9k stars, 2042 contributors & No & Big data analysis \\ \hline
    CloudQuery & CLI & 5.4k stars, 142 contributors & Yes & Transformation with DBT \\ \hline
    Dagster & Python and Web UI & 9.7k stars, 380 contributors & Only paid & Integrates with DBT and Airbyte \\ \hline
    DBT & CLI, SQL & 8.5k stars, 290 contributors & Yes & Integrates well with other tools \\ \hline
    Kestra & Localhost GUI with IDE & 5.4k stars, 194 contributors & Yes & Integrates with Airbyte and DBT \\ \hline
    Knime & GUI & 144 stars, 17 contributors on GitHub & Yes & Has own community platform \\ \hline
    Mage & GUI or own IDE, Python, R and SQL & 6.6k stars, 92 contributors & No &  \\ \hline
    Meltano & CLI & 1.5k stars, 120 contributors & No & Transformation with DBT \\ \hline
    Pentaho Community edition & Low-code GUI & 7.2k stars, 221 contributors & No &  \\ \hline
    Perfect & Python + monitoring UI & 14.1k stars, 227 contributors & Yes &  \\ \hline
    PipelineWise & CLI & 597 stars, 45 contributors & No & Requires Singer \\ \hline
    Python libraries\text{*} & Code & NA & No & Scheduling with cron \\ \hline
    R\_etl & R & NA & No & Dedicated R package \\ \hline
    Singer & CLI & $\sim$1.5k stars, $\sim$30 contributors & No & PipelineWise scheduling and monitoring \\ \hline
    \end{tabular}
    \caption{\text{*}The Python libraries include: Ethereum-etl, Pygrametl, Petl, Luigi}
\end{table}

If the tool uses code as its main way of building ETL pipelines, the programming language is mentioned. If the application is more low/no code, this is indicated in terms of the UI that is used. Sometimes both options are possible while other times a UI is only for monitoring the pipelines, not for building.

\end{document}
